\section{DRL}   


\begin{frame}{Deep Reinforcement Learning (DRL) Model}
    \tableofcontents[sectionstyle=show/hide, hideothersubsections]
    \centering
    \includegraphics[width=10cm]{images/drl_model.png}
\end{frame}

\subsection{Reinforcement Learning}
\begin{frame}{Reinforcement Learning}
\begin{columns}
    \begin{column}{0.45\textwidth}
    \begin{enumerate}
        \item Observes the states from the environment.
        \item Performs the action on the environment.
        \item Adjust its parameters based on the reward.
    \end{enumerate}
    \end{column}
    \begin{column}{0.45\textwidth}
      \centering
    \includegraphics[width=5cm]{images/rl_overview.png} \end{column}
\end{columns}
\end{frame}


\subsection{Value Optimization vs Policy Optimization}
\begin{frame}{Value Optimization vs Policy Optimization}
\end{frame}

\subsection{On-Policy vs Off-Policy}
\begin{frame}{On-Policy vs Off-Policy}
\end{frame}

\begin{frame}{Comparison}
\end{frame}

\subsection{Determinisitc Policy vs Stochastic Policy}
\begin{frame}{Determinisitc Policy vs Stochastic Policy}
\end{frame}

\subsection{Soft Actor-Critic}
\begin{frame}{Actor-Critic}
 \begin{columns}
    \begin{column}{0.45\textwidth}
    \begin{enumerate}
        \item Observes the states from the environment.
        \item Performs the action on the environment.
        \item Calculate TD error based on the reward.
        \item Adjust the parameters of Value function and the policy with the TD error.
    \end{enumerate}
    \end{column}
    \begin{column}{0.45\textwidth}
    \centering
    \includegraphics[width=5cm]{images/actor_critic.png}
 \end{column}
 \end{columns}
\end{frame}

\begin{frame}{Soft Actor-Critic}
We Use Soft Actor-Critic (SAC), as the RL model.
\begin{block}{SAC Overview}
\begin{itemize}
    \item Optimizes a stochastic policy in an off-policy way
    \item Optimizes the policy with \alert{entropy regularization} and takes entropy, measuring randomness in the policy, into account.
\end{itemize}
\end{block}
\begin{alertblock}{Reward Scale}
SAC is highly sensitive to the scaling of the reward. 
\begin{itemize}
    \item If reward scale is too small, the reward will fail to affect the model, and the policy will become nearly uniform.
    \item If reward scale is too large, the model will learn quickly and become deterministic, leaving no room for exploration.
\end{itemize}
\end{alertblock}
\end{frame}