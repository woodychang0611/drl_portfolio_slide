\begin{frame}{Overview}
\section{Implementation}
The model contain two main parts
\begin{itemize}
    \item Training Environment
    \item RL Model. 
\end{itemize}
\begin{center}
  \includegraphics[width=10cm]{images/context_diagram.png}  
\end{center}
\end{frame}




\begin{frame}{Market Features}
\begin{block}{Structured information}
\begin{itemize}
    \item Interest rates
    \item Commodity prices
    \item Currency exchange indexes
    \item Other Indexes
\end{itemize}
\end{block}
\begin{block}{Stationary data}
   Use directly as features
\end{block}
\begin{block}{Non-stationary data}
    Use statistics measures among 5, 20, and 60 days as features, including standard deviation, skewness, and kurtosis.
\end{block}
\end{frame}

\begin{frame}{Market Features}
\begin{tabular}{|| c| c||}
\hline
Description & Categories \\ \hline \hline
5-Year Treasury Constant Maturity Rate & Interest Rates\\ \hline
10-Year Treasury Constant Maturity Rate & Interest Rates\\ \hline
30-Year Treasury Constant Maturity Rate & Interest Rates\\ \hline
5-Year Breakeven Inflation Rate & Interest Rates\\ \hline
10-Year Breakeven Inflation Rate & Interest Rates\\ \hline
Crude Oil Prices: Brent - Europe &  Commodities\\ \hline
Gold Prices &  Commodities\\ \hline
CBOE Volatility Index (VIX) &  Indexes\\ \hline
US Dollar Index (USDX) &  Currencies\\ \hline
\end{tabular}
\end{frame}



\begin{frame}{Investment Universe}
\begin{block}{Goal}
Choose 10 investments with low covariances between each other; hence their combinations can yield better profits from the same risk level
\end{block}
\begin{block}{Selection Process}
    \begin{enumerate}
    \item Starts from the top 100 ETFs by Asset Under Management (AUM)
    \item Remove ones with trading records of less than 14 years.
    \item \label{itm:remove_items} Obtain the top two most correlated ETFs and remove the one with lower AUM
    \item Repeat Step\ref{itm:remove_items} iteratively until 10 ETFs are left
    \end{enumerate}
\end{block}
\end{frame}

\begin{frame}{Selection of ETFs}
    \begin{tabular}{|| c | c ||}
    \hline
    Symbol & Name  \\ \hline \hline
    SPY&SPDR S\&P 500 ETF \\ \hline
    AGG&iShares Core U.S. Aggregate Bond ETF \\ \hline
    BND&Vanguard Total Bond Market ETF \\ \hline
    GLD&SPDR Gold Trust \\ \hline
    LQD&iShares iBoxx \$ Investment Grade Corporate Bond ETF \\ \hline
    BSV&Vanguard Short-Term Bond ETF \\ \hline
    MBB&iShares MBS Bond ETF \\ \hline
    IGSB&iShares Short-Term Corporate Bond ETF \\ \hline
    SHY&iShares 1-3 Year Treasury Bond ETF \\ \hline
    SHV&iShares Short Treasury Bond ETF \\ \hline
    \end{tabular}
\end{frame}



\begin{frame}{Investor Preference}
\begin{block}{Acquire risk aversion}
Professional personnel or organization will acquire risk aversion from investors by survey or other techniques and interprets the inputs into comparable indicators between investors.
\\
\alert{This part is not in the scope of our thesis}
\end{block}
\begin{block}{Maximum Drawdown (MDD)}
MDD is one of the indicators to evaluate our system. However, optimizing any ratio using MDD directly has many challenges. Some researches indicate MDDs are outliers or imply optimizing MDD is troublesome. 
\\
\alert{We will not limit ourselves to use MDD while optimizing the model and consider other alternatives.}
\end{block}
\end{frame}



\begin{frame}{Reinforcement Learning (RL) Model}
We Use Soft Actor-Critic (SAC), as the RL model.
\begin{block}{SAC Overview}
\begin{itemize}
    \item Optimizes a stochastic policy in an off-policy way
    \item Optimizes the policy with \alert{entropy regularization} and takes entropy, measuring randomness in the policy, into account.
\end{itemize}
\end{block}
\begin{alertblock}{Reward Scale}
SAC is highly sensitive to the scaling of the reward. 
\begin{itemize}
    \item If reward scale is too small, the reward will fail to affect the model, and the policy will become nearly uniform.
    \item If reward scale is too large, the model will learn quickly and become deterministic, leaving no room for exploration.
\end{itemize}
\end{alertblock}
\end{frame}



\begin{frame}{Trading Environment}
A trading environment \alert{compatible with Open AI Gym}
\begin{center}
  \includegraphics[width=10cm]{images/trading_environment.png}
\end{center}
\end{frame}

\begin{frame}{Feature Extractor}
The feature extractor extracts states from the features.
\begin{enumerate}
    \item {
    Normalize the input features to mean 0 and standard division \(\sigma_{state}\),
        \[
            f^{'}_{i,t} = \frac{\sigma_{state} \times   (f_{i,t} -  \overline{f_i})}{\sigma_{f_i}}
        \]
    }
    \item{To avoid over-fitting, we add Gaussian distribution noise to the feature
            \[
            s_{i,t} = f^{'}_{i,t} + \mathcal{N}(0,\sigma_{noise}^2)
        \]
    }
\end{enumerate}
\alert{Validation results improved significantly after adding noise.}
\begin{center}
  \includegraphics[width=10cm]{images/compare_noise.png}
\end{center}
\end{frame}

\begin{frame}{Portfolio Builder}
The portfolio builder builds the portfolio from the investible universe and the action from the RL model.
\\
\begin{block}{Portfolio}
The portfolio F is an m dimension vector space of weights w.
\[
    F = \{ {w \in \mathbb{R} } \} ^m,
    \sum_{i=1}^m {w_i} =1
\]
\end{block}
\begin{block}{Weight}
Weight is a real number between 0 and 1. \alert{(No short position)}
\[
    w \in \mathbb{R} | 0 \leq w \leq 1 
\]
\end{block}
\end{frame}


\begin{frame}{Trading System}
The Trading System measures performances of the given portfolios
\begin{block}{Performances}
\begin{itemize}
    \item MDD
    \item CAGR
    \item Profit
    \item Wealth
\end{itemize}
\end{block}
These performances will be the input to calculate the reward or evaluate the performance of the system. 
\end{frame}


\begin{frame}{Reward Provider}
The Reward Provider provides the reward to the RL model based on the performance of the portfolio.
\begin{block}{Option 1: Penalty upon negative profits}
Penalty \(p_t-\theta\), upon negative profits exceeding the threshold \(\theta\). 
\[
p_t = \frac{w_t-w_{t-1}}{w_{t-1}}
, 
R_t = 
\begin{cases}
    p_t,&\text{if  }p_t > -\theta\\
    2p_t - \theta ,&\text{if  }p_t \leq  \theta
\end{cases}
\]
\end{block}
\begin{block}{Option 2: Penalty upon both positive and negative profits}
\[
R_t = 
\begin{cases}
    2p_t - \theta,&\text{if  }    $$|p_t|$$ > \theta\\
    p_t - \theta ,&\text{if  } $$|p_t|$$\leq  \theta
\end{cases}
\]
\end{block}
\alert{
The result shows unstable improvement in MDD. We observed that the optimization goes too well during training and generates a tiny amount of negative profits; therefore, these negative profits have minimal penalty effect on the model parameters.
}

\end{frame}

\begin{frame}{Reward Provider}
We then apply the penalty upon both positive and negative profits. This increases the stability significantly.

The threshold \(\theta\) will be the main adjustable parameter of our system. Adjusting it will enable the system to produce portfolios to meet different investors' risk tolerance.
\end{frame}
\begin{frame}{Reward Provider}
\begin{columns}
\begin{column}{0.25\textwidth}
\end{column}
\begin{column}{0.75\textwidth}
\begin{center}
      \includegraphics[height=7cm]{images/penalty_negtive_profits_compare.png}
\end{center}
\end{column}
\end{columns}
\end{frame}

\begin{frame}{Result}
\begin{columns}
\begin{column}{0.25\textwidth}
TEST
\end{column}
\begin{column}{0.75\textwidth}
\begin{center}
\includegraphics[height= 7.5cm]{images/crp_compare.png}
\end{center}
\end{column}
\end{columns}

\end{frame}
